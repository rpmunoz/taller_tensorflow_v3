{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de imágenes usando una red profunda - GPU\n",
    "\n",
    "**Profesor:** Roberto Muñoz <br />\n",
    "**E-mail:** <rmunoz@metricarts.com> <br />\n",
    "\n",
    "**Profesor:** Sebastián Arpón <br />\n",
    "**E-mail:** <rmunoz@metricarts.com> <br />\n",
    "\n",
    "\n",
    "Usaremos el dataset de perros y gatos. El dataset completo se puede descargar desde el link https://www.kaggle.com/c/dogs-vs-cats/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.data.python.ops import prefetching_ops\n",
    "\n",
    "opts = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\n",
    "conf = tf.ConfigProto(gpu_options=opts)\n",
    "\n",
    "tf.enable_eager_execution(config=conf)\n",
    "tf.device(\"/gpu:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Descargamos los datos\n",
    "\n",
    "Descargamos un dataset pequeño de perros y gatos. Lo descargamos ejecutando el siguiente código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_url = \"https://metriclearning.blob.core.windows.net/tensorflow/cats_and_dogs_medium.zip\"\n",
    "\n",
    "# Puede elegir entre diferentes datasets\n",
    "# Pequeño: https://metriclearning.blob.core.windows.net/tensorflow/cats_and_dogs_small.zip\n",
    "# Mediano: https://metriclearning.blob.core.windows.net/tensorflow/cats_and_dogs_medium.zip\n",
    "# Grande: https://metriclearning.blob.core.windows.net/tensorflow/cats_and_dogs_large.zip\n",
    "\n",
    "data_dir = \"data\"\n",
    "data_file = os.path.join(data_dir, \"cats_and_dogs_medium.zip\")\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "response = requests.get(data_url)\n",
    "response_data = response.content\n",
    "\n",
    "with open(data_file, 'wb') as f:\n",
    "    f.write(response_data)\n",
    "\n",
    "with open(data_file, 'rb') as f:\n",
    "    zf = zipfile.ZipFile(f)\n",
    "    zf.extractall(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos un resize de las imágenes a 150 x 150 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_dir(directory):\n",
    "    cats = glob.glob(os.path.join(directory,\"cats\") + '/*.jpg')\n",
    "    dogs = glob.glob(os.path.join(directory,\"dogs\") + '/*.jpg')\n",
    "    m_images = cats + dogs\n",
    "    \n",
    "    m_labels = []\n",
    "    m_labels.extend([CAT] * len(cats))\n",
    "    m_labels.extend([DOG] * len(dogs))\n",
    "    assert len(m_labels) == len(m_images)\n",
    "    LABELS_DIMENSIONS = 2\n",
    "    m_labels = tf.one_hot(m_labels, LABELS_DIMENSIONS)\n",
    "    print(\"Encontre %d imagenes y etiquetas en %s\" %(len(m_images),directory))\n",
    "    return m_images, m_labels\n",
    "\n",
    "def load_image(path_to_image, p_label):\n",
    "    m_label = p_label\n",
    "    m_image = tf.read_file(path_to_image)\n",
    "    m_image = tf.image.decode_jpeg(m_image)\n",
    "    m_image = tf.image.resize_images(m_image,(150,150))\n",
    "    m_image = m_image / 255\n",
    "    return m_image, m_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = \"data/cats_and_dogs_medium\"\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "val_dir = os.path.join(data_dir , \"validation\")\n",
    "\n",
    "CAT = 0\n",
    "DOG = 1\n",
    "\n",
    "print(\"Carpeta con imagenes para el entrenamiento: \", train_dir)\n",
    "print(\"Carpeta con imagenes para la validación: \", val_dir)\n",
    "print()\n",
    "\n",
    "train_images, train_labels = read_dir(train_dir)\n",
    "val_images, val_labels = read_dir(val_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i=np.random.randint(len(train_images))\n",
    "\n",
    "img, label = load_image(train_images[i], train_labels[i])\n",
    "\n",
    "print(img.shape)\n",
    "print(label)\n",
    "\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cargamos los datos en TensforFlow usando `tf.data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "buffer_size = 1000\n",
    "\n",
    "train_data_set = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "train_data_set = train_data_set.shuffle(buffer_size).map(load_image).batch(batch_size)\n",
    "#train_data_set = train_data_set.prefetch(buffer_size=4)\n",
    "train_data_set = train_data_set.apply(tf.contrib.data.prefetch_to_device(\"/gpu:0\", buffer_size=2))\n",
    "#train_data_set = train_data_set.apply(prefetching_ops.copy_to_device(\"/gpu:0\")) # Only works in TF > 1.10\n",
    "\n",
    "val_data_set = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
    "val_data_set = val_data_set.shuffle(buffer_size).map(load_image).batch(len(val_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable train_data_set contiene las imagenes del dataset de entrenamiento ordenadas en batches. Imprimimos en pantalla el tipo de dato y recorrimos cada batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(type(train_data_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(type(val_data_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for batch_i, (batch_image, batch_label) in enumerate(train_data_set):\n",
    "    print(\"Batch iteration: \", batch_i)\n",
    "    print(\"Batch image type: \", type(batch_image))\n",
    "    print(\"Batch image shape: \", batch_image.shape)\n",
    "    print(\"Batch label type: \", type(batch_label))\n",
    "    print(\"Batch label shape: \", batch_label.shape)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Definimos el modelo y lo entrenamos\n",
    "\n",
    "### Definimos la arquitectura de la red\n",
    "\n",
    "Usamos el método `AdamOptimizer` como funcion de optimización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    model.add(tf.layers.Conv2D(16, (5,5), activation=tf.nn.relu, input_shape=(150,150,3)))\n",
    "\n",
    "    model.add(tf.layers.MaxPooling2D(pool_size=(2,2),strides=2))\n",
    "\n",
    "    model.add(tf.layers.Conv2D(64,(3,3),activation=tf.nn.relu))\n",
    "    model.add(tf.layers.MaxPooling2D(pool_size=(2,2),strides=2))\n",
    "    model.add(tf.layers.Conv2D(128,(3,3),activation=tf.nn.relu))\n",
    "    model.add(tf.layers.MaxPooling2D(pool_size=(2,2),strides=2))\n",
    "    model.add(tf.layers.Conv2D(128,(3,3),activation=tf.nn.relu))\n",
    "    model.add(tf.layers.MaxPooling2D(pool_size=(2,2),strides=2))\n",
    "    model.add(tf.layers.Flatten())\n",
    "    model.add(tf.layers.Dense(512,activation=tf.nn.relu))\n",
    "    model.add(tf.layers.Dense(2,activation=tf.nn.softmax))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=tf.train.AdamOptimizer(),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "    \n",
    "model = create_model()    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definimos dos funciones para calcular las metricas de precision y recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precision(labels, predictions, weights=None):\n",
    "    conf_matrix = tf.confusion_matrix(labels, predictions, num_classes=2)\n",
    "    tp_and_fp = tf.reduce_sum(conf_matrix, axis=0)\n",
    "    tp = tf.diag_part(conf_matrix)\n",
    "    precision_scores = tp/(tp_and_fp)\n",
    "    if weights:\n",
    "        precision_score = tf.multiply(precision_scores, weights)/tf.reduce_sum(weights)\n",
    "    else:\n",
    "        precision_score = tf.reduce_mean(precision_scores)        \n",
    "    return precision_score\n",
    "\n",
    "def recall(labels, predictions, weights=None):\n",
    "    conf_matrix = tf.confusion_matrix(labels, predictions, num_classes=2)\n",
    "    tp_and_fn = tf.reduce_sum(conf_matrix, axis=1)\n",
    "    tp = tf.diag_part(conf_matrix)\n",
    "    recall_scores = tp/(tp_and_fn)\n",
    "    if weights:\n",
    "        recall_score = tf.multiply(recall_scores, weights)/tf.reduce_sum(weights)\n",
    "    else:\n",
    "        recall_score = tf.reduce_mean(recall_scores)        \n",
    "    return recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamos el modelo usando `train_on_batch()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint_dir='results'\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.mkdir(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "N_BATCHES = int(np.ceil(len(train_images)/batch_size))\n",
    "\n",
    "train_loss_epoch_list=[]\n",
    "train_acc_epoch_list=[]\n",
    "\n",
    "val_loss_epoch_list=[]\n",
    "val_accuracy_epoch_list=[]\n",
    "val_precision_epoch_list=[]\n",
    "val_recall_epoch_list=[]\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    progbar = tf.keras.utils.Progbar(N_BATCHES)\n",
    "    for (batch, (images, labels)) in enumerate(train_data_set):\n",
    "        train_loss, train_accuracy = model.train_on_batch(images, labels)\n",
    "        progbar.add(1, values=[(\"train loss\", train_loss), (\"acc\", train_accuracy)])\n",
    "\n",
    "    model.save_weights(os.path.join(checkpoint_dir, \"model_cats_and_dogs_{:04d}.ckpt\".format(epoch+1)))\n",
    "        \n",
    "    train_loss_epoch_list.append(float(train_loss))\n",
    "    train_acc_epoch_list.append(float(train_accuracy))\n",
    "    print('Entrenamiento época #%d\\t Loss: %.6f\\t Accuracy:  %.6f\\t'% (epoch+1, train_loss, train_accuracy))\n",
    "    \n",
    "    iterator = val_data_set.make_one_shot_iterator()\n",
    "    images, labels = iterator.get_next()  \n",
    "    val_loss, val_accuracy = model.evaluate(images, labels)\n",
    "    \n",
    "    preds_value = model.predict_classes(images)\n",
    "    labels_value = tf.argmax(labels, axis=1).numpy()\n",
    "       \n",
    "    val_precision = precision(labels_value, preds_value, weights=None)\n",
    "    val_recall = recall(labels_value, preds_value, weights=None)\n",
    "    \n",
    "    val_loss_epoch_list.append(float(val_loss))\n",
    "    val_accuracy_epoch_list.append(float(val_accuracy))\n",
    "    val_precision_epoch_list.append(float(val_precision))\n",
    "    val_recall_epoch_list.append(float(val_recall))\n",
    "    \n",
    "    print('Validacion época #%d\\t Loss: %.6f\\t Accuracy:  %.6f\\t' % (epoch+1, val_loss, val_accuracy))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graficamos la evolución de la función de costo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(8,6))\n",
    "\n",
    "x=x=np.arange(EPOCHS)+1\n",
    "ax.plot(x, train_loss_epoch_list, label=\"train\")\n",
    "\n",
    "plt.xlabel(\"Época\")\n",
    "plt.ylabel(\"Función de costo\")\n",
    "plt.legend(loc=1)\n",
    "plt.xlim(0,100)\n",
    "ax.grid(which='major', axis='x', linestyle='--')\n",
    "\n",
    "plt.title(\"Evolución de función de costo\")\n",
    "fig.savefig('results/loss_function_cats_and_dogs.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graficamos la evolución del accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(8,6))\n",
    "\n",
    "x=np.arange(EPOCHS)+1\n",
    "ax.plot(x, train_acc_epoch_list, label=\"train\")\n",
    "ax.plot(x, val_accuracy_epoch_list, label=\"validation\")\n",
    "\n",
    "plt.xlabel(\"Época\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc=1)\n",
    "plt.xlim(0,100)\n",
    "plt.ylim(0.4,1.05)\n",
    "ax.grid(which='major', axis='x', linestyle='--')\n",
    "\n",
    "plt.title(\"Evolución del accuracy\")\n",
    "fig.savefig('results/accuracy_cats_and_dogs.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(8,6))\n",
    "\n",
    "x=np.arange(EPOCHS)+1\n",
    "ax.plot(x, val_accuracy_epoch_list, label=\"accuracy\")\n",
    "ax.plot(x, val_recall_epoch_list, label=\"recall\")\n",
    "ax.plot(x, val_precision_epoch_list, label=\"precision\")\n",
    "\n",
    "plt.xlabel(\"Época\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend(loc=1)\n",
    "plt.xlim(0,100)\n",
    "plt.ylim(0.4,0.8)\n",
    "ax.grid(which='major', axis='x', linestyle='--')\n",
    "\n",
    "plt.title(\"Evolución de metricas\")\n",
    "fig.savefig('results/metrics_cats_and_dogs.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Revisamos el rendimiento del modelo en diferentes épocas\n",
    "\n",
    "Listamos los pesos que guardamos para cada época"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pattern = os.path.join(\"results/model_cats*.ckpt\")\n",
    "checkpoints = sorted(glob.glob(pattern))\n",
    "\n",
    "for file in checkpoints:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para validar el rendimiento del modelo usaremos el dataset de validación `val_data_set`\n",
    "\n",
    "Recuperamos los pesos para la **época 1** y evaluamos el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weight_file = 'results/model_cats_and_dogs_0001.ckpt'\n",
    "\n",
    "model = create_model()\n",
    "model.load_weights(weight_file)\n",
    "\n",
    "iterator = val_data_set.make_one_shot_iterator()\n",
    "images, labels = iterator.get_next()  \n",
    "\n",
    "loss, acc = model.evaluate(images, labels)\n",
    "\n",
    "print(\"Evaluation - Model accuracy: {:.4f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuperamos los pesos para la **época 30** y evaluamos el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_file = 'results/model_cats_and_dogs_0030.ckpt'\n",
    "\n",
    "model = create_model()\n",
    "model.load_weights(weight_file)\n",
    "\n",
    "iterator = val_data_set.make_one_shot_iterator()\n",
    "images, labels = iterator.get_next()  \n",
    "\n",
    "loss, acc = model.evaluate(images, labels)\n",
    "\n",
    "print(\"Evaluation - Model accuracy: {:.4f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Que tál si revisamos el gráfico de evolución del accuracy y elegimos un época intermedia?\n",
    "\n",
    "Vemos que la **época 6** tiene un accuracy más alto que el resto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_file = 'results/model_cats_and_dogs_0006.ckpt'\n",
    "\n",
    "model = create_model()\n",
    "model.load_weights(weight_file)\n",
    "\n",
    "iterator = val_data_set.make_one_shot_iterator()\n",
    "images, labels = iterator.get_next()  \n",
    "\n",
    "loss, acc = model.evaluate(images, labels)\n",
    "\n",
    "print(\"Evaluation - Model accuracy: {:.4f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitivamente los pesos obtenidos en la época 6 son los mejores que obtuvimos en este entrenamiento. Usaremos estos pesos para evaluar el rendimiento en imágenes nuevas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Descargamos imágenes nuevas y predecimos a qué clase pertencen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import io\n",
    "import imageio\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url=\"https://misanimales.com/wp-content/uploads/2014/12/gato-enfermo.jpg\"\n",
    "\n",
    "response = requests.get(url, stream=True)\n",
    "im = imageio.imread(io.BytesIO(response.content))\n",
    "\n",
    "plt.imshow(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisamos las dimensiones de la imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(type(im))\n",
    "print(im.shape)\n",
    "print(np.min(im))\n",
    "print(np.max(im))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para usar el método predict() de TensorFlow primero debemos cambiar el tamaño de la imagen a 150x150 pixels, luego transformar la imagen en un Tensor y finalmente agregar una dimensión extra al comienzo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "im_np = np.array(im)\n",
    "im_np = resize(im_np, (150,150))\n",
    "im_np = im_np.astype('float32')\n",
    "\n",
    "im_tf = tf.convert_to_tensor(im_np)\n",
    "im_tf = tf.expand_dims(im_tf, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.max(im_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.predict(im_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.predict_classes(im_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_url(url):\n",
    "    response = requests.get(url, stream=True)\n",
    "    im = imageio.imread(io.BytesIO(response.content))\n",
    "    \n",
    "    plt.imshow(im)\n",
    "    plt.show()\n",
    "    \n",
    "    im_np = np.array(im)\n",
    "    im_np = resize(im_np, (150,150))\n",
    "    im_np = im_np.astype('float32')\n",
    "\n",
    "    im_tf = tf.convert_to_tensor(im_np)\n",
    "    im_tf = tf.expand_dims(im_tf, 0)\n",
    "    \n",
    "    predict_class = model.predict_classes(im_tf)\n",
    "    \n",
    "    if predict_class == 0:\n",
    "        print(\"Predicción del modelo: Gato\")\n",
    "    elif predict_class == 1:\n",
    "        print(\"Predicción del modelo: Perro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url=\"https://misanimales.com/wp-content/uploads/2014/12/gato-enfermo.jpg\"\n",
    "predict_url(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url=\"https://s3-eu-west-1.amazonaws.com/barkibu-blog/blog+images/mi-perro-tiene-hipo-muy-seguido-que-le-pasa/perro-hipo-1.jpg\"\n",
    "predict_url(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
